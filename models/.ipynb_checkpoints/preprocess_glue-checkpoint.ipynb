{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7cb26d-9f2a-4ecd-ae8d-d2f635e423a4",
   "metadata": {},
   "source": [
    "# preprocessing glue for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fd00f21d-442d-487b-8900-6dde809e8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tiktoken\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b9c47c8-ac05-4abc-b921-61fbab0bcc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(task):\n",
    "    \n",
    "    dataset = load_dataset(\"nyu-mll/glue\", task)\n",
    "    num_labels = len(dataset['train'].features['label'].names)\n",
    "    \n",
    "    return dataset, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d77b24a3-fef8-4cee-9596-e3dd6e4d8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, num_labels = get_dataset('cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0f01bda7-2ffe-4237-985e-7f375d21bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_power_of_two(n):\n",
    "    return 1 << (n-1).bit_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4d4324a1-aadc-42c2-9bad-36cdab9e4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "pad_token = tokenizer.encode('<|endoftext|>', allowed_special=\"all\")[0]\n",
    "\n",
    "def tokenize_batch(sents):\n",
    "    tokens = tokenizer.encode_batch(sents, allowed_special = 'all')\n",
    "    padded = list(zip(*itertools.zip_longest(*tokens, fillvalue=pad_token)))\n",
    "    padded = np.array(padded)\n",
    "    return np.column_stack([padded, np.full(padded.shape[0], tokenizer.eot_token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dd2bc0ea-73e9-4591-beef-669f4b4e29bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1169,  3290, 50256, 50256, 50256],\n",
       "       [19963, 50256, 50256, 50256, 50256],\n",
       "       [ 1462,  1524, 50256, 50256, 50256],\n",
       "       [40838,   287,   262,  3650, 50256]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_batch(['the dog', 'went', 'to school', 'today in the store'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c11df5db-72fc-4327-a744-80041fbcd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_batches  = 6\n",
    "dataloader = DataLoader(dataset['train'], batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "299023b2-b723-4000-81e6-707cb440394e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['When it rains harder, how much faster a flow that appears in the river?', 'John seems to be easy to fool Ben.', 'Emma gave bad advice to Harriet.', 'John drinks coffee at 11, and Mary, tea at 10:30.'], 'label': tensor([0, 0, 1, 0]), 'idx': tensor([ 127, 4280, 6696, 7093])}\n",
      "{'sentence': ['That a review came out yesterday of this article is catastrophic.', 'the book of poems with a red cover by Robert Burns from Blackwell takes a very long time to read.', 'To train his horse would be desirable.', 'we need to provide two trees and.'], 'label': tensor([1, 1, 1, 1]), 'idx': tensor([1490, 5788, 4070, 7476])}\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f42ad77c-9f1f-49d7-bdcb-bea647e5ffcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63944d339d246a2aa30263c5408d93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing the splits:   0%|          | 0/8551 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c402e876d9444c19a86781fdec604cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing the splits:   0%|          | 0/1043 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6ccc0f23024aa9a6eeb06fe8555cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizing the splits:   0%|          | 0/1063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = dataset.map(\n",
    "    process,\n",
    "    remove_columns=['sentence'],\n",
    "    desc=\"tokenizing the splits\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d9d2be4-a301-4e25-ae24-2ba3130dd692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'idx': 0,\n",
       " 'ids': [5122,\n",
       "  2460,\n",
       "  1839,\n",
       "  470,\n",
       "  2822,\n",
       "  428,\n",
       "  3781,\n",
       "  11,\n",
       "  1309,\n",
       "  3436,\n",
       "  262,\n",
       "  1306,\n",
       "  530,\n",
       "  356,\n",
       "  18077,\n",
       "  13,\n",
       "  50256],\n",
       " 'len': 17}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for split, dset in tokenized.items():\n",
    "    arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "    filename = os.path.join(os.path.dirname(__file__), f'{split}.bin')\n",
    "    dtype = np.uint16 # (can do since enc.max_token_value == 50256 is < 2**16)\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "        # Batch together samples for faster write\n",
    "        batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        # Write into mmap\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "    arr.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
